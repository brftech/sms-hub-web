import { MessageData } from "./messageService";

export interface LLMRequest {
  message: string;
  context: string;
  businessDomain: string;
  userIntent: string;
  availableServices: string[];
  metadata?: Record<string, any>;
}

export interface LLMResponse {
  text: string;
  confidence: number;
  suggestedActions?: string[];
  metadata?: Record<string, any>;
}

export interface LLMConfig {
  provider: "openai" | "claude" | "zapier" | "custom";
  apiKey?: string;
  endpoint?: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

class LLMService {
  private config: LLMConfig;

  constructor(config: LLMConfig) {
    this.config = config;
  }

  /**
   * Send message to LLM and get response
   */
  async sendMessage(request: LLMRequest): Promise<LLMResponse> {
    try {
      switch (this.config.provider) {
        case "openai":
          return await this.sendToOpenAI(request);
        case "claude":
          return await this.sendToClaude(request);
        case "zapier":
          return await this.sendToZapier(request);
        case "custom":
          return await this.sendToCustomEndpoint(request);
        default:
          throw new Error(`Unsupported LLM provider: ${this.config.provider}`);
      }
    } catch (error) {
      console.error("LLM service error:", error);
      // Return fallback response
      return {
        text: "I'm having trouble processing your request right now. Please try again in a moment.",
        confidence: 0.1,
        suggestedActions: ["retry", "contact_support"],
      };
    }
  }

  /**
   * OpenAI integration
   */
  private async sendToOpenAI(request: LLMRequest): Promise<LLMResponse> {
    if (!this.config.apiKey) {
      throw new Error("OpenAI API key not configured");
    }

    const prompt = this.buildPrompt(request);

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${this.config.apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: this.config.model || "gpt-4",
        messages: [
          {
            role: "system",
            content: `You are a helpful AI assistant for Gnymble Premium Cigar Lounge. 
            You help customers with cigar recommendations, event information, pricing, and general inquiries.
            Be friendly, knowledgeable, and engaging. Use emojis sparingly but effectively.
            Always maintain the sophisticated yet approachable tone of a premium cigar lounge.`,
          },
          {
            role: "user",
            content: prompt,
          },
        ],
        temperature: this.config.temperature || 0.7,
        max_tokens: this.config.maxTokens || 150,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const data = await response.json();
    return {
      text: data.choices[0].message.content,
      confidence: 0.9,
      metadata: {
        model: data.model,
        usage: data.usage,
        provider: "openai",
      },
    };
  }

  /**
   * Claude integration
   */
  private async sendToClaude(request: LLMRequest): Promise<LLMResponse> {
    if (!this.config.apiKey) {
      throw new Error("Claude API key not configured");
    }

    const prompt = this.buildPrompt(request);

    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "x-api-key": this.config.apiKey,
        "Content-Type": "application/json",
        "anthropic-version": "2023-06-01",
      },
      body: JSON.stringify({
        model: this.config.model || "claude-3-sonnet-20240229",
        max_tokens: this.config.maxTokens || 150,
        messages: [
          {
            role: "user",
            content: prompt,
          },
        ],
        system: `You are a helpful AI assistant for Gnymble Premium Cigar Lounge. 
        You help customers with cigar recommendations, event information, pricing, and general inquiries.
        Be friendly, knowledgeable, and engaging. Use emojis sparingly but effectively.
        Always maintain the sophisticated yet approachable tone of a premium cigar lounge.`,
      }),
    });

    if (!response.ok) {
      throw new Error(`Claude API error: ${response.status}`);
    }

    const data = await response.json();
    return {
      text: data.content[0].text,
      confidence: 0.9,
      metadata: {
        model: data.model,
        usage: data.usage,
        provider: "claude",
      },
    };
  }

  /**
   * Zapier integration (webhook)
   */
  private async sendToZapier(request: LLMRequest): Promise<LLMResponse> {
    if (!this.config.endpoint) {
      throw new Error("Zapier webhook endpoint not configured");
    }

    const response = await fetch(this.config.endpoint, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        message: request.message,
        context: request.context,
        businessDomain: request.businessDomain,
        userIntent: request.userIntent,
        availableServices: request.availableServices,
        metadata: request.metadata,
        timestamp: new Date().toISOString(),
        source: "gnymble-website",
      }),
    });

    if (!response.ok) {
      throw new Error(`Zapier webhook error: ${response.status}`);
    }

    const data = await response.json();
    return {
      text:
        data.response ||
        data.text ||
        "Thanks for your message! I'm processing your request.",
      confidence: data.confidence || 0.8,
      suggestedActions: data.suggestedActions,
      metadata: {
        provider: "zapier",
        zapId: data.zapId,
        ...data.metadata,
      },
    };
  }

  /**
   * Custom endpoint integration
   */
  private async sendToCustomEndpoint(
    request: LLMRequest
  ): Promise<LLMResponse> {
    if (!this.config.endpoint) {
      throw new Error("Custom endpoint not configured");
    }

    const response = await fetch(this.config.endpoint, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(this.config.apiKey && {
          Authorization: `Bearer ${this.config.apiKey}`,
        }),
      },
      body: JSON.stringify(request),
    });

    if (!response.ok) {
      throw new Error(`Custom endpoint error: ${response.status}`);
    }

    const data = await response.json();
    return {
      text:
        data.response ||
        data.text ||
        "Thanks for your message! I'm processing your request.",
      confidence: data.confidence || 0.8,
      suggestedActions: data.suggestedActions,
      metadata: {
        provider: "custom",
        ...data.metadata,
      },
    };
  }

  /**
   * Build a comprehensive prompt for the LLM
   */
  private buildPrompt(request: LLMRequest): string {
    return `
Context: ${request.context}
Business Domain: ${request.businessDomain}
User Intent: ${request.userIntent}
Available Services: ${request.availableServices.join(", ")}

User Message: "${request.message}"

Please provide a helpful, engaging response that:
1. Addresses the user's inquiry directly
2. Maintains the sophisticated yet approachable tone of a premium cigar lounge
3. Offers relevant information about our services
4. Encourages further engagement
5. Uses 1-2 emojis if appropriate

Response:`;
  }

  /**
   * Update configuration
   */
  updateConfig(newConfig: Partial<LLMConfig>): void {
    this.config = { ...this.config, ...newConfig };
  }

  /**
   * Get current configuration
   */
  getConfig(): LLMConfig {
    return { ...this.config };
  }
}

// Default configuration - update with your actual API keys and endpoints
const defaultConfig: LLMConfig = {
  provider: "openai",
  apiKey: import.meta.env.VITE_OPENAI_API_KEY,
  model: "gpt-4",
  temperature: 0.7,
  maxTokens: 150,
};

export const llmService = new LLMService(defaultConfig);

// Export for easy configuration
export const configureLLM = (config: LLMConfig) => {
  llmService.updateConfig(config);
};
